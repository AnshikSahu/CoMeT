# Configuration file for Xeon X5550 Gainestown
# See http://en.wikipedia.org/wiki/Gainestown_(microprocessor)#Gainestown
# and http://ark.intel.com/products/37106

# Common config file for Nehalem core

[general]
enable_icache_modeling = true

[perf_model/core]
logical_cpus = 1 # number of SMT threads per core
type = interval
core_model = nehalem

[perf_model/core/interval_timer]
dispatch_width = 3
window_size = 84
num_outstanding_loadstores = 32

[perf_model/sync]
reschedule_cost = 1000

[caching_protocol]
type = parametric_dram_directory_msi

[perf_model/branch_predictor]
type = pentium_m
mispredict_penalty=8 # Reflects just the front-end portion (approx) of the penalty for Interval Simulation

[perf_model/tlb]
penalty = 30          # Page walk penalty in cycles

[perf_model/itlb]
size = 128            # Number of I-TLB entries
associativity = 4     # I-TLB associativity

[perf_model/dtlb]
size = 64             # Number of D-TLB entries
associativity = 4     # D-TLB associativity

[perf_model/stlb]
size = 512            # Number of second-level TLB entries
associativity = 4     # S-TLB associativity

[perf_model/cache]
levels = 2		# No L3.

[perf_model/l1_icache]
perfect = false
cache_size = 64
associativity = 2
address_hash = mask
replacement_policy = lru
data_access_time = 4
tags_access_time = 1
perf_model_type = parallel
writethrough = 0
shared_cores = 1

[perf_model/l1_dcache]
perfect = false
cache_size = 64
associativity = 2
address_hash = mask
replacement_policy = lru
data_access_time = 4
tags_access_time = 1
perf_model_type = parallel
writethrough = 0
shared_cores = 1

[perf_model/l2_cache]
perfect = false
cache_size = 512
associativity = 16
address_hash = mask
replacement_policy = lru
data_access_time = 8 # 8.something according to membench, -1 cycle L1 tag access time
# http://www.realworldtech.com/page.cfm?ArticleID=RWT040208182719&p=7
tags_access_time = 3
# Total neighbor L1/L2 access time is around 40/70 cycles (60-70 when it's coming out of L1)
writeback_time = 50 # L3 hit time will be added
perf_model_type = parallel
writethrough = 0
shared_cores = 1

[perf_model/l3_cache]
cache_block_size = 64
address_hash = mask
dvfs_domain = global # L1 and L2 run at core frequency (default), L3 is system frequency
prefetcher = none
writeback_time = 0

[clock_skew_minimization]
scheme = barrier

[clock_skew_minimization/barrier]
quantum = 100

[dvfs]
transition_latency = 2000 # In ns, "under 2 microseconds" according to http://download.intel.com/design/intarch/papers/323671.pdf (page 8)

[dvfs/simple]
cores_per_socket = 1

[power]
vdd = 1.2 # Volts
technology_node = 45 # nm

[perf_model/core]
frequency = 3.6

[perf_model/uncore]
frequency = 3.6

[perf_model/l3_cache]
perfect = false
cache_block_size = 64
cache_size = 8192
associativity = 16
address_hash = mask
replacement_policy = lru
data_access_time = 30 # 35 cycles total according to membench, +L1+L2 tag times
tags_access_time = 10
perf_model_type = parallel
writethrough = 0
shared_cores = 4

[perf_model/dram_directory]
# total_entries = number of entries per directory controller.
total_entries = 1048576
associativity = 16
directory_type = full_map

[perf_model/dram]
# -1 means that we have a number of distributed DRAM controllers (4 in this case)
num_controllers = 16 			# 16 channel memory
controllers_interleaving = 1		# 16 cores. 1 MCU per core.
# DRAM access latency in nanoseconds. Should not include L1-LLC tag access time, directory access time (14 cycles = 5.2 ns),
# or network time [(cache line size + 2*{overhead=40}) / network bandwidth = 18 ns]
# Membench says 175 cycles @ 2.66 GHz = 66 ns total
latency = 29				# From CACTI3DD, Bus delay from 
per_controller_bandwidth = 8            # In GB/s, as measured by core_validation-dram. 128(GB/s) for 16 channels => 8(GB/s) for 1 channel
chips_per_dimm = 8
dimms_per_controller = 4

[network]
memory_model_1 = bus
memory_model_2 = bus

[network/bus]
bandwidth = 25.6 # in GB/s. Actually, it's 12.8 GB/s per direction and per connected chip pair
ignore_local_traffic = true # Memory controllers are on-chip, so traffic from core0 to dram0 does not use the QPI links

[mem3D]
num_banks = 128
num_bank_address_bits = 7
num_lc = 16
bank_size = 64                                      # in Mb, 64MB partition = 512Mb partition, 8MB partition = 64Mb partition.
energy_per_read_access = 14.00           # in nJ. From CACTI3DD, 10,7.57,1.855*7 + 7.57 = 20.55........ Should be 15.15 nJ -> 3.7 pJ/bit 
energy_per_write_access = 14.00           # in nJ. From CACTI3DD, 10,7.57,1.855*7 + 7.57 = 20.55........ Should be 15.15 nJ -> 3.7 pJ/bit 
energy_per_refresh_access = 2.00    # in nJ. From CACTI3DD, 4
t_refi  = 7.8                                       # refresh interval time in uS
logic_core_power = 0.246
no_refesh_commands_in_t_refw = 8                    # in Kilo, 8K refresh commands are issued in a refresh window, one in each interval    
banks_in_x = 4
banks_in_y = 4
banks_in_z = 8
cores_in_x = 4
cores_in_y = 4
logic_cores_in_x = 4
logic_cores_in_y = 4
type_of_stack = 2.5D                #DDR, 3Dmem, 2.5D, 3D
#is_2_5d = true


#hotspot for 3D memory
[hotspot]
#tool_path = /mnt/disk1/hotspot_sniper/HotSpot-6.0_SLU_esweek_predict/
#work_path = /mnt/disk1/hotspot_sniper/PC/
hotspot_test_dir = default
tool_path = hotspot_tool/
config_path = config/
sampling_interval = 1000000
power_trace_file = power_mem.trace
full_power_trace_file = full_power_mem.trace
temperature_trace_file = temperature_mem.trace
full_temperature_trace_file = full_temperature_mem.trace
combined_temperature_trace_file = combined_temperature.trace
init_file_external = 4by4.init
init_file = temperature_mem.init
all_transient_file = all_transient_mem.init

hotspot_config_file  =  hotspot.config
hotspot_floorplan_file  =  hotspot/floorplan/4by4.flp
hotspot_floorplan_folder  = hotspot/floorplan
hotspot_layer_file  =   4by4.lcf

# Output Parameters for hotspot simulation
hotspot_steady_temp_file = ./steady_temperature_mem.log
hotspot_grid_steady_file = ./grid_steady_mem.log


#hotspot for cores
[hotspot_c]
tool_path = hotspot_c_tool/
config_path = config/
sampling_interval = 1000000
power_trace_file = power_core.trace
full_power_trace_file = full_power_core.trace
temperature_trace_file = temperature_core.trace
full_temperature_trace_file = full_temperature_core.trace
init_file_external = core_external.init
init_file = temperature_core.init
#cpi_stack_file = InstantaneousCPIStack.log
#cpi_stack_periodic_file = PeriodicCPIStack.log
#all_transient_file = all_transient_core.init

hotspot_config_file  =  hotspot.core.config
hotspot_floorplan_file  =  hotspot/floorplan/4x4_manycore.flp
hotspot_floorplan_folder  = hotspot/floorplan
#hotspot_layer_file  =   4by4.lcf

# Output Parameters for hotspot simulation
hotspot_steady_temp_file = ./steady_temperature_core.log
hotspot_grid_steady_file = ./grid_steady_core.log

